{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepmahajan70/HPC/blob/main/CUDA_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using CUDA in Colab\n",
        "\n",
        "By Dr. Nileshchandra Pikle <br>\n",
        "Assistant Professor,<br>\n",
        "Indian Institute of Information Technology, Nagpur<br>\n",
        "A Certified CUDA programming Instructor by NVIDIA<br>\n",
        "Email: nilesh.pikle@bmail.com<br>\n",
        "Contact: 7276834418"
      ],
      "metadata": {
        "id": "jV9dI_gF7923"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before executing this command make surte to select Runtime -> change runtime type -> select GPU from drop down\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLlrhYNB8HlB",
        "outputId": "7d0ad125-845c-4bc8-f689-df702ab4257e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run the given command to install a small extension to run nvcc from the Notebook cell**"
      ],
      "metadata": {
        "id": "ViIri02q8kXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADecyEOD8flJ",
        "outputId": "43a36ce6-7f51-46e6-c5e4-c85494791a9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-whtse108\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-whtse108\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4288 sha256=12bb8c449889dafce0cde2571d4a31538f945b617ad47287169ef6f3daafe2e8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bklp4c8y/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load the extension using the code given below:**"
      ],
      "metadata": {
        "id": "TtWdGgWO82DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJl0bK2H82e0",
        "outputId": "9f8089a9-cf9b-4bce-8caf-23608ef6b076"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Execute the code given below to check if CUDA is working or not.**\n",
        "To run the code in your notebook, add the %%cu extension at the beginning of your code.\n"
      ],
      "metadata": {
        "id": "mpdwkzQA8_9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda --name MatrixMult.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "void helloCPU()\n",
        "{\n",
        "  printf(\"Hello from the CPU.\\n\");\n",
        "}\n",
        "__global__ void helloGPU()\n",
        "{\n",
        "  printf(\"Hello also from the GPU.\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "  helloCPU();\n",
        "  // First #thread blocks Second = # threads per block\n",
        "  helloGPU<<<2,32>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mMEXTmAk9CYY",
        "outputId": "318d7e94-f430-4f49-c739-387dfc09d0ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/MatrixMult.cu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compiling to create an executable file**"
      ],
      "metadata": {
        "id": "fMgAvu7aP4zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc /content/src/MatrixMult.cu -o /content/src/MatrixMult"
      ],
      "metadata": {
        "id": "oHBo1-z0P3M3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Execute the output file**"
      ],
      "metadata": {
        "id": "bNqs64pCQZ6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57jev6KxQdUB",
        "outputId": "3b9f7560-cdc7-4d0d-f8cf-2c42219b5a54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd src"
      ],
      "metadata": {
        "id": "284p9XLHQ5GV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "# Note that still we are not in the src directory even we used cd command\n",
        "# So always use complete path of executable while executing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjvWaHUBQ8Vr",
        "outputId": "43a30523-f915-434c-b27d-8a8fd00a78f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/src/MatrixMult"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhGA4CI5Rgyq",
        "outputId": "2e84e121-3e0b-4df2-aeb8-8565fb8e71d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from the CPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/src/MatrixMult"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPMu5qIIUPk3",
        "outputId": "5add8d9a-d551-4372-ca9b-e1453204280e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from the CPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n",
            "Hello also from the GPU.\n"
          ]
        }
      ]
    }
  ]
}